{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transforms for image normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing with mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "# Load the Sign Language MNIST dataset\n",
    "train_data = pd.read_csv('sign_mnist_train.csv')\n",
    "test_data = pd.read_csv('sign_mnist_test.csv')\n",
    "\n",
    "# Split features (images) and labels\n",
    "train_labels = train_data['label'].values\n",
    "train_images = train_data.drop(columns=['label']).values.reshape(-1, 28, 28, 1)\n",
    "\n",
    "test_labels = test_data['label'].values\n",
    "test_images = test_data.drop(columns=['label']).values.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Custom dataset class for loading data into PyTorch\n",
    "class SignLanguageMNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Apply transform only if the transform exists and the image is a NumPy array\n",
    "        if isinstance(image, torch.Tensor):  # If it's already a tensor, skip transforming\n",
    "            if self.transform is not None:\n",
    "                image = image.squeeze(0)  # Remove the channel dimension before applying ToTensor\n",
    "                image = transforms.ToPILImage()(image)  # Convert tensor back to PIL for transforms\n",
    "                image = self.transform(image)  # Apply any additional transforms\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Convert train and test images to PyTorch tensors and normalize\n",
    "def numpy_to_tensor(images):\n",
    "    # Normalize images by dividing by 255.0 and reshaping to (batch_size, channels, height, width)\n",
    "    images = images / 255.0  # Scale to range [0, 1]\n",
    "    return torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)  # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "# Convert train and test images to tensors\n",
    "train_images_tensor = numpy_to_tensor(train_images)\n",
    "test_images_tensor = numpy_to_tensor(test_images)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SignLanguageMNISTDataset(train_images_tensor, train_labels, transform=transform)\n",
    "test_dataset = SignLanguageMNISTDataset(test_images_tensor, test_labels, transform=transform)\n",
    "\n",
    "# Split the dataset into sequential tasks (A-G, H-M, etc.)\n",
    "class_indices = {\n",
    "    'A-G': list(range(0, 7)),\n",
    "    'H-M': list(range(7, 13)),\n",
    "    'N-Y': list(range(13, 24)),\n",
    "}\n",
    "\n",
    "# Function to filter the dataset for specific class indices\n",
    "def filter_dataset(dataset, labels, class_list):\n",
    "    indices = [i for i, label in enumerate(labels) if label in class_list]\n",
    "    filtered_images = dataset.images[indices]\n",
    "    filtered_labels = labels[indices]\n",
    "    return SignLanguageMNISTDataset(filtered_images, filtered_labels, transform=transform)\n",
    "\n",
    "# Filter datasets for the first task (A-G)\n",
    "train_task_1 = filter_dataset(train_dataset, train_labels, class_indices['A-G'])\n",
    "test_task_1 = filter_dataset(test_dataset, test_labels, class_indices['A-G'])\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_loader_1 = data.DataLoader(train_task_1, batch_size=64, shuffle=True)\n",
    "test_loader_1 = data.DataLoader(test_task_1, batch_size=64, shuffle=False)\n",
    "\n",
    "# Filter datasets for the first task (H-M)\n",
    "train_task_2 = filter_dataset(train_dataset, train_labels, class_indices['H-M'])\n",
    "test_task_2 = filter_dataset(test_dataset, test_labels, class_indices['H-M'])\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_loader_2 = data.DataLoader(train_task_2, batch_size=64, shuffle=True)\n",
    "test_loader_2 = data.DataLoader(test_task_2, batch_size=64, shuffle=False)\n",
    "\n",
    "# Filter datasets for the first task (N-Y)\n",
    "train_task_3 = filter_dataset(train_dataset, train_labels, class_indices['N-Y'])\n",
    "test_task_3 = filter_dataset(test_dataset, test_labels, class_indices['N-Y'])\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_loader_3 = data.DataLoader(train_task_3, batch_size=64, shuffle=True)\n",
    "test_loader_3 = data.DataLoader(test_task_3, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1 (A-G)\n",
      "Epoch 1/5, Loss: 0.5549492282284931\n",
      "Epoch 2/5, Loss: 0.018570050168089755\n",
      "Epoch 3/5, Loss: 0.003991767989225255\n",
      "Epoch 4/5, Loss: 0.0012935314918935592\n",
      "Epoch 5/5, Loss: 0.0007957747852629866\n",
      "Accuracy: 99.21194525093323%\n",
      "Training on Task 2 (H-M)\n",
      "Epoch 1/5, Loss: 1.4258639277699827\n",
      "Epoch 2/5, Loss: 0.0078831060487904\n",
      "Epoch 3/5, Loss: 0.0026444566319696605\n",
      "Epoch 4/5, Loss: 0.0014840812655165792\n",
      "Epoch 5/5, Loss: 0.0010025783154891212\n",
      "Accuracy: 98.37153196622437%\n",
      "Training on Task 3 (N-Y)\n",
      "Epoch 1/5, Loss: 1.3842451707186068\n",
      "Epoch 2/5, Loss: 0.06430541890600294\n",
      "Epoch 3/5, Loss: 0.013289945895153591\n",
      "Epoch 4/5, Loss: 0.005487438705042187\n",
      "Epoch 5/5, Loss: 0.002899472751225546\n",
      "Accuracy: 92.67412486466979%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=24):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the feature maps\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN(num_classes=24)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(train_loader, model, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(test_loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Train and evaluate on Task 1 (A-G)\n",
    "print(\"Training on Task 1 (A-G)\")\n",
    "train_model(train_loader_1, model, criterion, optimizer)\n",
    "evaluate_model(test_loader_1, model)\n",
    "\n",
    "# Train and evaluate on Task 2 (H-M)\n",
    "print(\"Training on Task 2 (H-M)\")\n",
    "train_model(train_loader_2, model, criterion, optimizer)\n",
    "evaluate_model(test_loader_2, model)\n",
    "\n",
    "# Train and evaluate on Task 3 (N-Y)\n",
    "print(\"Training on Task 3 (N-Y)\")\n",
    "train_model(train_loader_3, model, criterion, optimizer)\n",
    "evaluate_model(test_loader_3, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1 (A-G)\n",
      "Epoch 1/5, Loss: 3.26875426022967\n",
      "Epoch 2/5, Loss: 0.12702572764443956\n",
      "Epoch 3/5, Loss: 0.05781017981223331\n",
      "Epoch 4/5, Loss: 0.03141088018286203\n",
      "Epoch 5/5, Loss: 0.018137903022002584\n",
      "Accuracy: 95.64496059726255%\n",
      "Training on Task 2 (H-M)\n",
      "Epoch 1/5, Loss: 1.413072870790281\n",
      "Epoch 2/5, Loss: 0.09706354439681904\n",
      "Epoch 3/5, Loss: 0.03711329928641631\n",
      "Epoch 4/5, Loss: 0.018400687364522706\n",
      "Epoch 5/5, Loss: 0.011408783828797326\n",
      "Accuracy: 95.41616405307599%\n",
      "Training on Task 3 (N-Y)\n",
      "Epoch 1/5, Loss: 18.359993273136663\n",
      "Epoch 2/5, Loss: 18.27793525247013\n",
      "Epoch 3/5, Loss: 18.01444570690978\n",
      "Epoch 4/5, Loss: 17.863793550753126\n",
      "Epoch 5/5, Loss: 17.754735675512574\n",
      "Accuracy: 88.5239985564778%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# Define the EWC loss function\n",
    "class EWC:\n",
    "    def __init__(self, model, dataloader, criterion):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.criterion = criterion\n",
    "        self.ewc_loss = 0\n",
    "        self.fisher_information = None\n",
    "        self.old_params = None\n",
    "\n",
    "    def compute_fisher_information(self):\n",
    "        fisher_information = {name: torch.zeros_like(param) for name, param in self.model.named_parameters()}\n",
    "        self.model.eval()\n",
    "        for images, labels in self.dataloader:\n",
    "            self.model.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            for name, param in self.model.named_parameters():\n",
    "                fisher_information[name] += param.grad**2 / len(self.dataloader)\n",
    "        self.fisher_information = fisher_information\n",
    "\n",
    "    def compute_ewc_loss(self):\n",
    "        ewc_loss = 0\n",
    "        if self.old_params and self.fisher_information:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                old_param = self.old_params[name]\n",
    "                fisher = self.fisher_information[name]\n",
    "                ewc_loss += torch.sum(fisher * (param - old_param)**2)\n",
    "        return ewc_loss\n",
    "\n",
    "    def update_old_params(self):\n",
    "        self.old_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "\n",
    "# Initialize the EWC object\n",
    "ewc = EWC(model, train_loader_1, criterion)\n",
    "\n",
    "# Train the model with EWC\n",
    "def train_model_with_ewc(train_loader, model, criterion, optimizer, ewc=None, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if ewc:\n",
    "                ewc_loss = ewc.compute_ewc_loss()\n",
    "                loss += ewc_loss  # Add EWC loss to the original loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    # Update EWC parameters after training\n",
    "    if ewc:\n",
    "        ewc.update_old_params()\n",
    "\n",
    "# Train and evaluate on Task 1 (A-G)\n",
    "print(\"Training on Task 1 (A-G)\")\n",
    "train_model_with_ewc(train_loader_1, model, criterion, optimizer, ewc)\n",
    "evaluate_model(test_loader_1, model)\n",
    "\n",
    "# Compute Fisher Information for Task 1\n",
    "ewc.compute_fisher_information()\n",
    "\n",
    "# Train and evaluate on Task 2 (H-M)\n",
    "print(\"Training on Task 2 (H-M)\")\n",
    "train_model_with_ewc(train_loader_2, model, criterion, optimizer, ewc)\n",
    "evaluate_model(test_loader_2, model)\n",
    "\n",
    "# Compute Fisher Information for Task 2\n",
    "ewc.compute_fisher_information()\n",
    "\n",
    "# Train and evaluate on Task 3 (N-Y)\n",
    "print(\"Training on Task 3 (N-Y)\")\n",
    "train_model_with_ewc(train_loader_3, model, criterion, optimizer, ewc)\n",
    "evaluate_model(test_loader_3, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Task 1 (A-G)\n",
      "Epoch 1/5, Loss: 0.5781094817798739\n",
      "Epoch 2/5, Loss: 0.013782972110096704\n",
      "Epoch 3/5, Loss: 0.0026500108055308585\n",
      "Epoch 4/5, Loss: 0.001122368277629745\n",
      "Epoch 5/5, Loss: 0.0008130953678464871\n",
      "Accuracy: 99.46%\n",
      "Training on Task 2 (H-M)\n",
      "Epoch 1/5, Loss: 1.2985429563347928\n",
      "Epoch 2/5, Loss: 0.006754650483013724\n",
      "Epoch 3/5, Loss: 0.0025290687378649386\n",
      "Epoch 4/5, Loss: 0.001522875528528609\n",
      "Epoch 5/5, Loss: 0.0009784747010350904\n",
      "Accuracy: 98.55%\n",
      "Training on Task 3 (N-Y)\n",
      "Epoch 1/5, Loss: 1.7921590620250094\n",
      "Epoch 2/5, Loss: 0.11168481344722357\n",
      "Epoch 3/5, Loss: 0.02500317555477879\n",
      "Epoch 4/5, Loss: 0.008728468039657409\n",
      "Epoch 5/5, Loss: 0.004683416283703135\n",
      "Accuracy: 89.21%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.20967159870084"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# Train the model without any CL strategy\n",
    "def train_model_naive(train_loader, model, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model_naive = CNN(num_classes=24)  # Make sure the model class (CNN) is defined\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_naive.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate on Task 1 (A-G)\n",
    "print(\"Training on Task 1 (A-G)\")\n",
    "train_model_naive(train_loader_1, model_naive, criterion, optimizer)\n",
    "evaluate_model(test_loader_1, model_naive)\n",
    "\n",
    "# Train and evaluate on Task 2 (H-M)\n",
    "print(\"Training on Task 2 (H-M)\")\n",
    "train_model_naive(train_loader_2, model_naive, criterion, optimizer)\n",
    "evaluate_model(test_loader_2, model_naive)\n",
    "\n",
    "# Train and evaluate on Task 3 (N-Y)\n",
    "print(\"Training on Task 3 (N-Y)\")\n",
    "train_model_naive(train_loader_3, model_naive, criterion, optimizer)\n",
    "evaluate_model(test_loader_3, model_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the trained model (make sure the model is in evaluation mode)\n",
    "model_naive.eval()\n",
    "\n",
    "# Define a transformation to preprocess the frame\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "# Function to preprocess a frame\n",
    "def preprocess_frame(frame):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize to 28x28\n",
    "    resized = cv2.resize(gray, (28, 28))\n",
    "    # Normalize and convert to tensor\n",
    "    tensor = transform(resized).unsqueeze(0)  # Add batch dimension\n",
    "    return tensor\n",
    "\n",
    "# Open a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Define region of interest for hand detection (you may need to adjust this)\n",
    "    roi = frame[50:300, 50:300]  # Example coordinates for ROI\n",
    "    \n",
    "    # Convert to HSV color space for color segmentation (skin color detection)\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define the skin color range in HSV\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    \n",
    "    # Threshold the HSV image to get only skin colors\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Find the largest contour which is assumed to be the hand\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(max_contour)\n",
    "        \n",
    "        # Extract the hand region and preprocess\n",
    "        hand_img = roi[y:y+h, x:x+w]\n",
    "        input_tensor = preprocess_frame(hand_img)\n",
    "        \n",
    "        # Run the hand image through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model_naive(input_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Get the predicted label\n",
    "        predicted_label = chr(predicted.item() + ord('A'))  # Assuming labels 0-23 map to 'A'-'Y'\n",
    "\n",
    "        # Draw bounding box and label on the original frame\n",
    "        cv2.rectangle(frame, (50 + x, 50 + y), (50 + x + w, 50 + y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Predicted: {predicted_label}', (50 + x, 50 + y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Real-time Sign Language Recognition', frame)\n",
    "    \n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
